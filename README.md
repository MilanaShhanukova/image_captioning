# ðŸ†Ž Image_captioning ðŸ“·

This is a repository for the final project in [Deep Learning School](https://www.dlschool.org/) about NLP spring 2021. Here I try to solve image captioning problem. 

The whole idea of image captioning is to create the description of one picture. In two architectures below I am using the method of first giving the picture to the model and then making generation according to it and the previously decoded state.

Creating this project I've used three articles: 
[Overview of image captioning models](https://downloads.hindawi.com/journals/cin/2020/3062706.pdf)
[Show, Attend and Tell](https://arxiv.org/pdf/1502.03044.pdf)
[Metrics for image captionin](https://arxiv.org/pdf/1612.07600.pdf)

In a few scipts you can find: 
* model - with and without attention mechanism 
* training pipeline
* metrics calculation 

All of the parts of the models and its representation can be found here in [image_captioning_project.ipynb](https://github.com/MilanaShhanukova/image_captioning/blob/master/image_captioning_project%20(2).ipynb)

All of the training reports with metrics can be found here: 

[model without attention](https://wandb.ai/miana/image_captioning/reports/Report-Model-without-attention--Vmlldzo4Mjc3MTA)

[model with attention](https://wandb.ai/miana/attention/reports/Model-with-attention---Vmlldzo4MzAzNjY)

![image](https://user-images.githubusercontent.com/60469549/124648187-a12a4380-de9f-11eb-8ac2-77dc9d755145.png)



